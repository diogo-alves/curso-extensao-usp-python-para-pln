{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 4 - NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBtErk7SrQ9Z",
        "outputId": "5ce94887-b676-4da0-d78f-b48b0d197c2e"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# prepara o ambiente\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = Path('/content/drive/MyDrive/Cursos/Python para Processamento de Linguagem Natural (USP)/Aula 4')\n",
        "FILEPATH_CORPUS = BASE_DIR / 'corpus_teste.txt'\n",
        "\n",
        "# atualiza o beautifulsoup para a última versão\n",
        "!pip install -U beautifulsoup4"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4) (2.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4CDhRMLg11S"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CIaCDp8gvkS"
      },
      "source": [
        "## Instalação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acy3FuX4Lfmu",
        "outputId": "d5780770-f7aa-4514-d87b-c8677372cd10"
      },
      "source": [
        " # instalando o nltk (já pré instalado no colab)\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTHYh9yVg5m0"
      },
      "source": [
        "## Download dos pacotes de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEcOC9-rNX5Z",
        "outputId": "84b71128-7959-4dc4-d841-e99089fafefb"
      },
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "# downloads dos pacotes (dados)\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUvG43Twhsmv"
      },
      "source": [
        "## Extração de palavras de um corpus\n",
        "Um corpus é nada mais que um conjunto de dados linguísticos, ou seja, um texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iTf4TBVQdZ0",
        "outputId": "107bec42-92c6-4508-8cff-5f696bb7f133"
      },
      "source": [
        "words = nltk.corpus.mac_morpho.words()\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jersei', 'atinge', 'média', 'de', 'Cr$', '1,4', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVTy21tXh3Lm"
      },
      "source": [
        "## Extração de sentenças de um corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Okxg1CRvLA",
        "outputId": "717ab90e-4ea0-4f84-ce62-9faa30108bb6"
      },
      "source": [
        "# retorna a primeira frase do texto\n",
        "sents = nltk.corpus.mac_morpho.sents()[0]\n",
        "print(sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jersei', 'atinge', 'média', 'de', 'Cr$', '1,4', 'milhão', 'em', 'a', 'venda', 'de', 'a', 'Pinhal', 'em', 'São', 'Paulo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGn4h8SCiKww"
      },
      "source": [
        "## Classificação gramatical de palavras em um corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcMJj2wIS-GW",
        "outputId": "97a42559-6d67-478b-f402-70ea56ac23bf"
      },
      "source": [
        "# retorna as palavras classificadas por classe gramatical (N-Nome, V-Verbo, etc)\n",
        "nltk.corpus.mac_morpho.tagged_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjoCrq1bifoq"
      },
      "source": [
        "## Classificação gramatical de sentenças em um corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9yo0NDFTrNK",
        "outputId": "5cb6f442-333b-404e-cfdf-afd47c792eb1"
      },
      "source": [
        "# retorna uma sentença com as palavras já classificadas gramaticalmente\n",
        "tagged_words = nltk.corpus.mac_morpho.tagged_sents()[0]\n",
        "print(tagged_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ('de', 'PREP'), ('Cr$', 'CUR'), ('1,4', 'NUM'), ('milhão', 'N'), ('em', 'PREP|+'), ('a', 'ART'), ('venda', 'N'), ('de', 'PREP|+'), ('a', 'ART'), ('Pinhal', 'NPROP'), ('em', 'PREP'), ('São', 'NPROP'), ('Paulo', 'NPROP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z65VTeQ7ipPa"
      },
      "source": [
        "## Tokenização de um texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrPpFAQiaZjq",
        "outputId": "d4db5d47-14a2-4d2f-de8e-b407aa513574"
      },
      "source": [
        "text = (\n",
        "    'Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o '\n",
        "    'New York Giants anotou o touchdown decisivo e derrubou o favorito New '\n",
        "    'England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.'\n",
        ")\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Com', 'um', 'passe', 'de', 'Eli', 'Manning', 'para', 'Plaxico', 'Burress', 'a', '39', 'segundos', 'do', 'fim', ',', 'o', 'New', 'York', 'Giants', 'anotou', 'o', 'touchdown', 'decisivo', 'e', 'derrubou', 'o', 'favorito', 'New', 'England', 'Patriots', 'por', '17', 'a', '14', 'neste', 'domingo', ',', 'em', 'Glendale', ',', 'no', 'Super', 'Bowl', 'XLII', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_9esfbjDw0"
      },
      "source": [
        "## Remoção de pontuações em um texto com RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4tlKdNqc4p3",
        "outputId": "05c2afe0-c2b0-48a7-fb80-bf93ec86613e"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "text = (\n",
        "    'Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o '\n",
        "    'New York Giants anotou o touchdown decisivo e derrubou o favorito New '\n",
        "    'England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.'\n",
        ")\n",
        "# tokeniza o texto sem pontuações\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Com', 'um', 'passe', 'de', 'Eli', 'Manning', 'para', 'Plaxico', 'Burress', 'a', '39', 'segundos', 'do', 'fim', 'o', 'New', 'York', 'Giants', 'anotou', 'o', 'touchdown', 'decisivo', 'e', 'derrubou', 'o', 'favorito', 'New', 'England', 'Patriots', 'por', '17', 'a', '14', 'neste', 'domingo', 'em', 'Glendale', 'no', 'Super', 'Bowl', 'XLII']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urqXql6LgWgz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bufxZlw1jSLV"
      },
      "source": [
        "## Remoção de pontuação e numerais em um texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta9ByPyvjugy",
        "outputId": "11b3d770-1f9e-4680-a750-9822eaf23674"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "text = (\n",
        "    'Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o '\n",
        "    'New York Giants anotou o touchdown decisivo e derrubou o favorito New '\n",
        "    'England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.'\n",
        ")\n",
        "# tokenizar o texto sem pontuações ou numerais\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Com', 'um', 'passe', 'de', 'Eli', 'Manning', 'para', 'Plaxico', 'Burress', 'a', 'segundos', 'do', 'fim', 'o', 'New', 'York', 'Giants', 'anotou', 'o', 'touchdown', 'decisivo', 'e', 'derrubou', 'o', 'favorito', 'New', 'England', 'Patriots', 'por', 'a', 'neste', 'domingo', 'em', 'Glendale', 'no', 'Super', 'Bowl', 'XLII']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxXB5-TYjcqh"
      },
      "source": [
        "## Exibição das palavras mais frequentes em um texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaJ-KErZnd-t",
        "outputId": "44a64c2a-66fa-4197-e72e-338086f19f6d"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "NUM_FREQ_WORDS = 3\n",
        "text = (\n",
        "    'Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o '\n",
        "    'New York Giants anotou o touchdown decisivo e derrubou o favorito New '\n",
        "    'England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.'\n",
        ")\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "# gera lista de  palavras mais frequentes no texto\n",
        "frequence = nltk.FreqDist(tokens)\n",
        "print(frequence.most_common(NUM_FREQ_WORDS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('o', 3), ('a', 2), ('New', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3GCvHfNjxk6"
      },
      "source": [
        "## Exibição das palavras mais frequentes em um corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avrt_XLFpJx-",
        "outputId": "7f490dc5-20c6-4a1f-d0ec-be7997728dab"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "with open(FILEPATH_CORPUS) as f:\n",
        "    corpus = f.read()\n",
        "    tokenizer = RegexpTokenizer('[a-zA-Z]\\w*')\n",
        "    tokens = tokenizer.tokenize(corpus)\n",
        "    tokens = map(str.lower, tokens)\n",
        "    # exibe palavras mais frequentes do corpus\n",
        "    frequence = nltk.FreqDist(tokens)\n",
        "    print(frequence.most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('de', 34), ('o', 33), ('a', 28), ('e', 21), ('para', 16), ('jardas', 15), ('os', 13), ('com', 13), ('na', 13), ('do', 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrD7YGY4j9E6"
      },
      "source": [
        "## Listagem das stop words em um texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fReqwfqW1JpZ",
        "outputId": "c4775078-d67f-4389-b507-9cb6254da0af"
      },
      "source": [
        "# stopwords são palavras irrelevantes pro contexto do texto (artigos, preposições, conjunções, etc)\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGLDzq3n1-jw",
        "outputId": "3433372f-71e6-4e80-c831-aa758a536159"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "with open(FILEPATH_CORPUS) as f:\n",
        "    corpus = f.read()\n",
        "    tokenizer = RegexpTokenizer('[a-zA-Z]\\w*')\n",
        "    tokens = tokenizer.tokenize(corpus)\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "    # exibe palavras mais frequentes do corpus removendo as stopwords\n",
        "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
        "    frequence = nltk.FreqDist(tokens)\n",
        "    print(frequence.most_common())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('jardas', 15), ('giants', 11), ('patriots', 10), ('manning', 10), ('linha', 10), ('bola', 7), ('vez', 6), ('zone', 6), ('new', 5), ('passe', 5), ('york', 5), ('brady', 5), ('avanço', 5), ('super', 4), ('bowl', 4), ('england', 4), ('eli', 4), ('segundos', 4), ('fim', 4), ('touchdown', 4), ('time', 4), ('nova', 4), ('jogo', 4), ('achou', 4), ('end', 4), ('título', 3), ('história', 3), ('primeira', 3), ('chegou', 3), ('posse', 3), ('logo', 3), ('passes', 3), ('curtos', 3), ('red', 3), ('campanha', 3), ('ataque', 3), ('avançou', 3), ('período', 3), ('xlii', 2), ('temporada', 2), ('plaxico', 2), ('burress', 2), ('anotou', 2), ('tom', 2), ('companhia', 2), ('fazer', 2), ('nfl', 2), ('ano', 2), ('vitória', 2), ('porém', 2), ('máximo', 2), ('field', 2), ('goal', 2), ('entrou', 2), ('maroney', 2), ('deixando', 2), ('chegaram', 2), ('seguinte', 2), ('amani', 2), ('toomer', 2), ('novamente', 2), ('steve', 2), ('smith', 2), ('soltou', 2), ('dois', 2), ('cometeu', 2), ('fumble', 2), ('voltou', 2), ('pontuar', 2), ('chance', 2), ('desta', 2), ('tentou', 2), ('conseguiu', 2), ('jogada', 2), ('kevin', 2), ('incrível', 2), ('jogadas', 2), ('moss', 2), ('batem', 1), ('azarões', 1), ('acabam', 1), ('invencibilidade', 1), ('ficam', 1), ('h07m', 1), ('atualizado', 1), ('h49m', 1), ('decisivo', 1), ('derrubou', 1), ('favorito', 1), ('neste', 1), ('domingo', 1), ('glendale', 1), ('resultado', 1), ('maiores', 1), ('zebras', 1), ('acabou', 1), ('perfeita', 1), ('esperavam', 1), ('levantar', 1), ('troféu', 1), ('sofrer', 1), ('derrota', 1), ('ficará', 1), ('irmãos', 1), ('quarterbacks', 1), ('triunfam', 1), ('temporadas', 1), ('consecutivas', 1), ('passado', 1), ('peyton', 1), ('irmão', 1), ('indianapolis', 1), ('colts', 1), ('partida', 1), ('começaram', 1), ('mostraram', 1), ('iriam', 1), ('alongar', 1), ('posses', 1), ('misturando', 1), ('corridas', 1), ('brandon', 1), ('jacobs', 1), ('entanto', 1), ('parou', 1), ('lawrence', 1), ('tynes', 1), ('converteu', 1), ('abrir', 1), ('placar', 1), ('ficaram', 1), ('m54s', 1), ('campo', 1), ('frio', 1), ('retorno', 1), ('kickoff', 1), ('running', 1), ('back', 1), ('laurence', 1), ('boa', 1), ('posição', 1), ('graças', 1), ('penalidade', 1), ('interferência', 1), ('linebacker', 1), ('antonio', 1), ('pierce', 1), ('alcançaram', 1), ('jarda', 1), ('chão', 1), ('primeiro', 1), ('pareciam', 1), ('rumo', 1), ('virada', 1), ('sofreram', 1), ('revés', 1), ('passou', 1), ('ellis', 1), ('hobbs', 1), ('aproveitou', 1), ('tomou', 1), ('defesa', 1), ('manteve', 1), ('equilibrado', 1), ('sacks', 1), ('seguidos', 1), ('forçaram', 1), ('punt', 1), ('recuperaram', 1), ('provou', 1), ('ser', 1), ('outra', 1), ('decepção', 1), ('sofreu', 1), ('sack', 1), ('conseguindo', 1), ('ltima', 1), ('marcar', 1), ('antes', 1), ('intervalo', 1), ('segundo', 1), ('sacado', 1), ('tomaram', 1), ('longo', 1), ('ltimos', 1), ('sucesso', 1), ('continuou', 1), ('amarrado', 1), ('terceiro', 1), ('quarto', 1), ('defesas', 1), ('levando', 1), ('melhor', 1), ('sobre', 1), ('ataques', 1), ('nica', 1), ('técnico', 1), ('bill', 1), ('bellichick', 1), ('optou', 1), ('quarta', 1), ('descida', 1), ('jabar', 1), ('gaffney', 1), ('completar', 1), ('ltimo', 1), ('começou', 1), ('arrasador', 1), ('tight', 1), ('boss', 1), ('deixou', 1), ('outro', 1), ('lançamento', 1), ('marcou', 1), ('duas', 1), ('david', 1), ('tyree', 1), ('pegou', 1), ('cinco', 1), ('anotar', 1), ('virar', 1), ('hora', 1), ('decisão', 1), ('funcionar', 1), ('série', 1), ('variados', 1), ('wes', 1), ('welker', 1), ('randy', 1), ('faulk', 1), ('seguidas', 1), ('vezes', 1), ('chegar', 1), ('m45s', 1), ('quarterback', 1), ('conectou', 1), ('desmarcou', 1), ('ficou', 1), ('livre', 1), ('lateral', 1), ('direita', 1), ('fãs', 1), ('comemoravam', 1), ('inesperado', 1), ('aconteceu', 1), ('marcadores', 1), ('seguravam', 1), ('camisa', 1), ('corrida', 1), ('lançou', 1), ('wide', 1), ('receiver', 1), ('bem', 1), ('marcado', 1), ('saltou', 1), ('recepção', 1), ('quatro', 1), ('conseguir', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wInGK_9SqFS"
      },
      "source": [
        "# Lista de exercícios 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6VMSKMhStos"
      },
      "source": [
        "## 1. Crie uma variável com a string “ instituto de ciências matemáticas e de computação” e faça:\n",
        "a. Concatene (adicione) uma outra string chamada “usp”\n",
        "\n",
        "b. Concatene (adicione) uma outra informação: 2021\n",
        "\n",
        "c. Verifique o tamanho da nova string (com as informações adicionadas das questões a e b), com referência a caracteres e espaços\n",
        "\n",
        "d. Transforme a string inteiramente em maiúsculo\n",
        "\n",
        "e. Transforme a string inteiramente em minúsculo\n",
        "\n",
        "f. Retire o espaço que está no início da string e imprima a string\n",
        "\n",
        "g. Substitua todas as letras ‘a’ por ‘x’\n",
        "\n",
        "h. Separe a string em palavras únicas\n",
        "\n",
        "i. Verifique quantas palavras existem na string\n",
        "\n",
        "j. Separe a string por meio da palavra “de”\n",
        "\n",
        "k. Verifique agora quantas palavras/frases foram formadasquando houve a separação pela palavra “de”\n",
        "\n",
        "l. Junte as palavras que foram separadas (pode usar a separação resultante da questão h ou j)\n",
        "\n",
        "m. Junte as palavras que foram separadas, mas agora separadas por uma barra  invertida,  não por  espaços  (pode  usar  a  separação  resultante  da questão h ou j)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXxtYvRzTllz",
        "outputId": "e3e8003b-c81e-4d2b-83e5-44b8e10c7bff"
      },
      "source": [
        "text = ' instituto de ciências matemáticas e de computação'\n",
        "text += 'usp'\n",
        "print('a:', text)\n",
        "text += '2021'\n",
        "print('b:', text)\n",
        "print('c:', len(text))\n",
        "print('d:', text.upper())\n",
        "print('e:', text.lower())\n",
        "print('f:', text.strip())\n",
        "print('g:', text.replace('a', 'x'))\n",
        "words = text.split()\n",
        "print('h:', words)\n",
        "print('i:', len(words))\n",
        "words = text.split('de')\n",
        "print('j:', words)\n",
        "print('k:', len(words))\n",
        "print('l:', ''.join(words))\n",
        "print('m:', '\\\\'.join(words))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:  instituto de ciências matemáticas e de computaçãousp\n",
            "b:  instituto de ciências matemáticas e de computaçãousp2021\n",
            "c: 57\n",
            "d:  INSTITUTO DE CIÊNCIAS MATEMÁTICAS E DE COMPUTAÇÃOUSP2021\n",
            "e:  instituto de ciências matemáticas e de computaçãousp2021\n",
            "f: instituto de ciências matemáticas e de computaçãousp2021\n",
            "g:  instituto de ciêncixs mxtemáticxs e de computxçãousp2021\n",
            "h: ['instituto', 'de', 'ciências', 'matemáticas', 'e', 'de', 'computaçãousp2021']\n",
            "i: 7\n",
            "j: [' instituto ', ' ciências matemáticas e ', ' computaçãousp2021']\n",
            "k: 3\n",
            "l:  instituto  ciências matemáticas e  computaçãousp2021\n",
            "m:  instituto \\ ciências matemáticas e \\ computaçãousp2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyx8wa5daBu7"
      },
      "source": [
        "## 2. Escolha um corpus qualquer (pode ser o que foi utilizado na  aula)  e, usando as funções do NLTK, faça:\n",
        "a. Tokenize o corpus inteiro (palavras, números e pontuações)\n",
        "\n",
        "b. Verifique a quantidade de tokens do corpus\n",
        "\n",
        "c. Tokenize o corpus apenas por suas palavras \n",
        "\n",
        "d. Verifique a quantidade de palavras do corpus\n",
        "\n",
        "e. Verifique a frequência de palavras no corpus\n",
        "\n",
        "f. Verifique quais são as 5, 10 e 15 palavras mais frequentes do corpus\n",
        "\n",
        "g. Extraia as stopwords do NLTK (não do corpus ainda)\n",
        "\n",
        "h. Verifique a frequência dos tokens sem stopwords do corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjj1nyQiEzEx"
      },
      "source": [
        "from urllib.request import urlopen, Request\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def get_news_from_meu_timao(url: str) -> str:\n",
        "    \"\"\"Extrai notícia do site Meu Timão\"\"\"\n",
        "    request = Request(url, headers={'User-Agent': '*'}) \n",
        "    response = urlopen(request)\n",
        "    html_doc = response.read()\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "    paragraphs = soup.select('.texto_noticia > p:not(.tags)')\n",
        "    return ' '.join(p.text for p in paragraphs)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8kFkHhBzsgS",
        "outputId": "7f3a4b6e-9986-4720-f087-0b97dfc7102c"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "URL = 'https://www.meutimao.com.br/noticias-do-corinthians/393200/cassio-abre-idade-para-aposentadoria-e-afirma-ronaldo-sempre-sera-o-maior-goleiro-no-corinthians'\n",
        "\n",
        "\n",
        "corpus = get_news_from_meu_timao(URL)\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "print('a:', tokens)\n",
        "print('b:', len(tokens))\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "words = tokenizer.tokenize(corpus)\n",
        "print('c:', words)\n",
        "print('d:', len(words))\n",
        "frequence = nltk.FreqDist(words)\n",
        "print('e:', frequence.most_common())\n",
        "print('f:', frequence.most_common(5), frequence.most_common(10), frequence.most_common(15))\n",
        "pt_stopwords = stopwords.words('portuguese')\n",
        "print('g:', pt_stopwords)\n",
        "words = [word for word in words if word not in pt_stopwords]\n",
        "frequence = nltk.FreqDist(words)\n",
        "print('h:', frequence.most_common())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: ['O', 'goleiro', 'Cássio', 'quer', 'jogar', 'até', 'os', '40', 'anos', 'de', 'idade', 'e', ',', 'claro', ',', 'espera', 'que', 'isso', 'aconteça', 'com', 'a', 'camisa', 'do', 'Corinthians', '.', 'Foi', 'isso', 'que', 'ele', 'disse', 'na', 'tarde', 'desta', 'sexta-feira', ',', 'no', 'CT', 'Joaquim', 'Grava', ',', 'ao', 'ser', 'lembrado', 'que', ',', 'caso', 'entre', 'na', 'partida', 'contra', 'o', 'Red', 'Bull', 'Bragantino', ',', 'pelo', 'Brasileiro', ',', 'irá', 'igualar', 'o', 'número', 'de', 'jogos', 'de', 'Claudio', 'Christovam', 'de', 'Pinho', ',', 'o', 'maior', 'artilheiro', 'da', 'história', 'do', 'clube', '.', '``', 'Tenho', 'em', 'mente', 'jogar', 'até', 'os', '40', 'anos', ',', 'espero', 'que', 'eu', 'possa', 'ficar', 'o', 'maior', 'tempo', 'possível', 'vestindo', 'a', 'camisa', 'do', 'Corinthians', '.', 'A', 'gente', 'olha', 'para', 'trás', ',', 'vê', 'tudo', 'que', 'construiu', ',', 'o', 'tanto', 'que', 'trabalhou', '.', 'Me', 'sinto', 'feliz', \"''\", ',', 'contou', 'o', 'Gigante', ',', 'abrindo', 'um', 'sorriso', 'ao', 'constatar', 'a', 'história', 'que', 'vem', 'construindo', 'no', 'clube', '.', '``', 'Feliz', ',', 'né', ',', 'a', 'gente', 'fica', 'feliz', 'com', 'os', 'números', ',', 'décima', 'temporada', 'pelo', 'Corinthians', ',', 'atingindo', 'meta', ',', 'números', ',', 'a', 'gente', 'fica', 'feliz', '.', 'Não', 'alcançamos', 'esses', 'números', 'sozinho', ',', 'se', 'eu', 'fosse', 'falar', 'ia', 'ficar', 'o', 'dia', 'inteiro', 'aqui', '.', 'Lógico', 'que', 'me', 'sinto', 'muito', 'feliz', ',', 'lisonjeado', 'de', 'vestir', 'essa', 'camisa', ',', 'o', 'maior', 'clube', 'do', 'futebol', 'brasileiro', '.', 'Só', 'quem', 'jogou', 'no', 'Corinthians', 'sabe', 'o', 'que', 'é', 'vestir', 'essa', 'camisa', ',', 'representar', 'uma', 'nação', \"''\", ',', 'continuou', 'o', 'jogador', '.', 'Sempre', 'mantendo', 'o', 'respeito', 'por', 'quem', 'veio', 'antes', 'dele', 'no', 'clube', ',', 'Cássio', 'ainda', 'tratou', 'do', 'tema', 'de', 'estar', 'entre', 'lendas', 'da', 'história', 'alvinegra', '.', 'Com', 'os', '550', 'jogos', 'a', 'serem', 'completados', ',', 'ele', 'fica', 'a', 'pouco', 'mais', 'de', '50', 'de', 'Ronaldo', 'Giovanelli', ',', 'outro', 'grande', 'nome', 'da', 'posição', 'que', 'ainda', 'rivaliza', 'consigo', '.', 'E', ',', 'para', 'Cássio', ',', 'nada', 'o', 'fará', 'superar', 'o', 'ídolo', 'das', 'décadas', 'de', '80', 'e', '90', '.', '``', 'Fico', 'muito', 'lisonjeado', 'com', 'tudo', 'o', 'que', 'está', 'acontecendo', ',', 'mas', 'prefiro', 'viver', 'o', 'dia', 'a', 'dia', ',', 'me', 'dedicar', 'ao', 'máximo', '.', 'Posso', 'até', 'passar', 'o', 'Ronaldo', 'em', 'jogos', ',', 'títulos', ',', 'mas', ',', 'para', 'mim', ',', 'ele', 'vai', 'ser', 'sempre', 'o', 'maior', 'goleiro', 'da', 'história', 'do', 'Corinthians', \"''\", ',', 'assegurou', ',', 'antes', 'de', 'fechar', 'em', 'tom', 'de', 'união', 'de', 'lendas', '.', '``', 'Quem', 'ganha', 'é', 'o', 'Corinthians', ',', 'o', 'Ronaldo', 'honrou', 'a', 'camisa', 'do', 'Corinthians', 'lá', 'atrás', ',', 'Julio', ',', 'Dida', ',', 'Gilmar', ',', 'peço', 'desculpas', 'por', 'não', 'falar', 'o', 'nome', 'de', 'todos', ',', 'mas', 'eles', 'que', 'construíram', 'essa', 'história', 'também', \"''\", ',', 'concluiu', '.']\n",
            "b: 408\n",
            "c: ['O', 'goleiro', 'Cássio', 'quer', 'jogar', 'até', 'os', '40', 'anos', 'de', 'idade', 'e', 'claro', 'espera', 'que', 'isso', 'aconteça', 'com', 'a', 'camisa', 'do', 'Corinthians', 'Foi', 'isso', 'que', 'ele', 'disse', 'na', 'tarde', 'desta', 'sexta', 'feira', 'no', 'CT', 'Joaquim', 'Grava', 'ao', 'ser', 'lembrado', 'que', 'caso', 'entre', 'na', 'partida', 'contra', 'o', 'Red', 'Bull', 'Bragantino', 'pelo', 'Brasileiro', 'irá', 'igualar', 'o', 'número', 'de', 'jogos', 'de', 'Claudio', 'Christovam', 'de', 'Pinho', 'o', 'maior', 'artilheiro', 'da', 'história', 'do', 'clube', 'Tenho', 'em', 'mente', 'jogar', 'até', 'os', '40', 'anos', 'espero', 'que', 'eu', 'possa', 'ficar', 'o', 'maior', 'tempo', 'possível', 'vestindo', 'a', 'camisa', 'do', 'Corinthians', 'A', 'gente', 'olha', 'para', 'trás', 'vê', 'tudo', 'que', 'construiu', 'o', 'tanto', 'que', 'trabalhou', 'Me', 'sinto', 'feliz', 'contou', 'o', 'Gigante', 'abrindo', 'um', 'sorriso', 'ao', 'constatar', 'a', 'história', 'que', 'vem', 'construindo', 'no', 'clube', 'Feliz', 'né', 'a', 'gente', 'fica', 'feliz', 'com', 'os', 'números', 'décima', 'temporada', 'pelo', 'Corinthians', 'atingindo', 'meta', 'números', 'a', 'gente', 'fica', 'feliz', 'Não', 'alcançamos', 'esses', 'números', 'sozinho', 'se', 'eu', 'fosse', 'falar', 'ia', 'ficar', 'o', 'dia', 'inteiro', 'aqui', 'Lógico', 'que', 'me', 'sinto', 'muito', 'feliz', 'lisonjeado', 'de', 'vestir', 'essa', 'camisa', 'o', 'maior', 'clube', 'do', 'futebol', 'brasileiro', 'Só', 'quem', 'jogou', 'no', 'Corinthians', 'sabe', 'o', 'que', 'é', 'vestir', 'essa', 'camisa', 'representar', 'uma', 'nação', 'continuou', 'o', 'jogador', 'Sempre', 'mantendo', 'o', 'respeito', 'por', 'quem', 'veio', 'antes', 'dele', 'no', 'clube', 'Cássio', 'ainda', 'tratou', 'do', 'tema', 'de', 'estar', 'entre', 'lendas', 'da', 'história', 'alvinegra', 'Com', 'os', '550', 'jogos', 'a', 'serem', 'completados', 'ele', 'fica', 'a', 'pouco', 'mais', 'de', '50', 'de', 'Ronaldo', 'Giovanelli', 'outro', 'grande', 'nome', 'da', 'posição', 'que', 'ainda', 'rivaliza', 'consigo', 'E', 'para', 'Cássio', 'nada', 'o', 'fará', 'superar', 'o', 'ídolo', 'das', 'décadas', 'de', '80', 'e', '90', 'Fico', 'muito', 'lisonjeado', 'com', 'tudo', 'o', 'que', 'está', 'acontecendo', 'mas', 'prefiro', 'viver', 'o', 'dia', 'a', 'dia', 'me', 'dedicar', 'ao', 'máximo', 'Posso', 'até', 'passar', 'o', 'Ronaldo', 'em', 'jogos', 'títulos', 'mas', 'para', 'mim', 'ele', 'vai', 'ser', 'sempre', 'o', 'maior', 'goleiro', 'da', 'história', 'do', 'Corinthians', 'assegurou', 'antes', 'de', 'fechar', 'em', 'tom', 'de', 'união', 'de', 'lendas', 'Quem', 'ganha', 'é', 'o', 'Corinthians', 'o', 'Ronaldo', 'honrou', 'a', 'camisa', 'do', 'Corinthians', 'lá', 'atrás', 'Julio', 'Dida', 'Gilmar', 'peço', 'desculpas', 'por', 'não', 'falar', 'o', 'nome', 'de', 'todos', 'mas', 'eles', 'que', 'construíram', 'essa', 'história', 'também', 'concluiu']\n",
            "d: 342\n",
            "e: [('o', 20), ('de', 13), ('que', 12), ('a', 9), ('do', 7), ('Corinthians', 7), ('camisa', 5), ('história', 5), ('os', 4), ('no', 4), ('maior', 4), ('da', 4), ('clube', 4), ('feliz', 4), ('Cássio', 3), ('até', 3), ('com', 3), ('ele', 3), ('ao', 3), ('jogos', 3), ('em', 3), ('gente', 3), ('para', 3), ('fica', 3), ('números', 3), ('dia', 3), ('essa', 3), ('Ronaldo', 3), ('mas', 3), ('goleiro', 2), ('jogar', 2), ('40', 2), ('anos', 2), ('e', 2), ('isso', 2), ('na', 2), ('ser', 2), ('entre', 2), ('pelo', 2), ('eu', 2), ('ficar', 2), ('tudo', 2), ('sinto', 2), ('falar', 2), ('me', 2), ('muito', 2), ('lisonjeado', 2), ('vestir', 2), ('quem', 2), ('é', 2), ('por', 2), ('antes', 2), ('ainda', 2), ('lendas', 2), ('nome', 2), ('O', 1), ('quer', 1), ('idade', 1), ('claro', 1), ('espera', 1), ('aconteça', 1), ('Foi', 1), ('disse', 1), ('tarde', 1), ('desta', 1), ('sexta', 1), ('feira', 1), ('CT', 1), ('Joaquim', 1), ('Grava', 1), ('lembrado', 1), ('caso', 1), ('partida', 1), ('contra', 1), ('Red', 1), ('Bull', 1), ('Bragantino', 1), ('Brasileiro', 1), ('irá', 1), ('igualar', 1), ('número', 1), ('Claudio', 1), ('Christovam', 1), ('Pinho', 1), ('artilheiro', 1), ('Tenho', 1), ('mente', 1), ('espero', 1), ('possa', 1), ('tempo', 1), ('possível', 1), ('vestindo', 1), ('A', 1), ('olha', 1), ('trás', 1), ('vê', 1), ('construiu', 1), ('tanto', 1), ('trabalhou', 1), ('Me', 1), ('contou', 1), ('Gigante', 1), ('abrindo', 1), ('um', 1), ('sorriso', 1), ('constatar', 1), ('vem', 1), ('construindo', 1), ('Feliz', 1), ('né', 1), ('décima', 1), ('temporada', 1), ('atingindo', 1), ('meta', 1), ('Não', 1), ('alcançamos', 1), ('esses', 1), ('sozinho', 1), ('se', 1), ('fosse', 1), ('ia', 1), ('inteiro', 1), ('aqui', 1), ('Lógico', 1), ('futebol', 1), ('brasileiro', 1), ('Só', 1), ('jogou', 1), ('sabe', 1), ('representar', 1), ('uma', 1), ('nação', 1), ('continuou', 1), ('jogador', 1), ('Sempre', 1), ('mantendo', 1), ('respeito', 1), ('veio', 1), ('dele', 1), ('tratou', 1), ('tema', 1), ('estar', 1), ('alvinegra', 1), ('Com', 1), ('550', 1), ('serem', 1), ('completados', 1), ('pouco', 1), ('mais', 1), ('50', 1), ('Giovanelli', 1), ('outro', 1), ('grande', 1), ('posição', 1), ('rivaliza', 1), ('consigo', 1), ('E', 1), ('nada', 1), ('fará', 1), ('superar', 1), ('ídolo', 1), ('das', 1), ('décadas', 1), ('80', 1), ('90', 1), ('Fico', 1), ('está', 1), ('acontecendo', 1), ('prefiro', 1), ('viver', 1), ('dedicar', 1), ('máximo', 1), ('Posso', 1), ('passar', 1), ('títulos', 1), ('mim', 1), ('vai', 1), ('sempre', 1), ('assegurou', 1), ('fechar', 1), ('tom', 1), ('união', 1), ('Quem', 1), ('ganha', 1), ('honrou', 1), ('lá', 1), ('atrás', 1), ('Julio', 1), ('Dida', 1), ('Gilmar', 1), ('peço', 1), ('desculpas', 1), ('não', 1), ('todos', 1), ('eles', 1), ('construíram', 1), ('também', 1), ('concluiu', 1)]\n",
            "f: [('o', 20), ('de', 13), ('que', 12), ('a', 9), ('do', 7)] [('o', 20), ('de', 13), ('que', 12), ('a', 9), ('do', 7), ('Corinthians', 7), ('camisa', 5), ('história', 5), ('os', 4), ('no', 4)] [('o', 20), ('de', 13), ('que', 12), ('a', 9), ('do', 7), ('Corinthians', 7), ('camisa', 5), ('história', 5), ('os', 4), ('no', 4), ('maior', 4), ('da', 4), ('clube', 4), ('feliz', 4), ('Cássio', 3)]\n",
            "g: ['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n",
            "h: [('Corinthians', 7), ('camisa', 5), ('história', 5), ('maior', 4), ('clube', 4), ('feliz', 4), ('Cássio', 3), ('jogos', 3), ('gente', 3), ('fica', 3), ('números', 3), ('dia', 3), ('Ronaldo', 3), ('goleiro', 2), ('jogar', 2), ('40', 2), ('anos', 2), ('ser', 2), ('ficar', 2), ('tudo', 2), ('sinto', 2), ('falar', 2), ('lisonjeado', 2), ('vestir', 2), ('antes', 2), ('ainda', 2), ('lendas', 2), ('nome', 2), ('O', 1), ('quer', 1), ('idade', 1), ('claro', 1), ('espera', 1), ('aconteça', 1), ('Foi', 1), ('disse', 1), ('tarde', 1), ('desta', 1), ('sexta', 1), ('feira', 1), ('CT', 1), ('Joaquim', 1), ('Grava', 1), ('lembrado', 1), ('caso', 1), ('partida', 1), ('contra', 1), ('Red', 1), ('Bull', 1), ('Bragantino', 1), ('Brasileiro', 1), ('irá', 1), ('igualar', 1), ('número', 1), ('Claudio', 1), ('Christovam', 1), ('Pinho', 1), ('artilheiro', 1), ('Tenho', 1), ('mente', 1), ('espero', 1), ('possa', 1), ('tempo', 1), ('possível', 1), ('vestindo', 1), ('A', 1), ('olha', 1), ('trás', 1), ('vê', 1), ('construiu', 1), ('tanto', 1), ('trabalhou', 1), ('Me', 1), ('contou', 1), ('Gigante', 1), ('abrindo', 1), ('sorriso', 1), ('constatar', 1), ('vem', 1), ('construindo', 1), ('Feliz', 1), ('né', 1), ('décima', 1), ('temporada', 1), ('atingindo', 1), ('meta', 1), ('Não', 1), ('alcançamos', 1), ('sozinho', 1), ('ia', 1), ('inteiro', 1), ('aqui', 1), ('Lógico', 1), ('futebol', 1), ('brasileiro', 1), ('Só', 1), ('jogou', 1), ('sabe', 1), ('representar', 1), ('nação', 1), ('continuou', 1), ('jogador', 1), ('Sempre', 1), ('mantendo', 1), ('respeito', 1), ('veio', 1), ('tratou', 1), ('tema', 1), ('estar', 1), ('alvinegra', 1), ('Com', 1), ('550', 1), ('serem', 1), ('completados', 1), ('pouco', 1), ('50', 1), ('Giovanelli', 1), ('outro', 1), ('grande', 1), ('posição', 1), ('rivaliza', 1), ('consigo', 1), ('E', 1), ('nada', 1), ('fará', 1), ('superar', 1), ('ídolo', 1), ('décadas', 1), ('80', 1), ('90', 1), ('Fico', 1), ('acontecendo', 1), ('prefiro', 1), ('viver', 1), ('dedicar', 1), ('máximo', 1), ('Posso', 1), ('passar', 1), ('títulos', 1), ('mim', 1), ('vai', 1), ('sempre', 1), ('assegurou', 1), ('fechar', 1), ('tom', 1), ('união', 1), ('Quem', 1), ('ganha', 1), ('honrou', 1), ('lá', 1), ('atrás', 1), ('Julio', 1), ('Dida', 1), ('Gilmar', 1), ('peço', 1), ('desculpas', 1), ('todos', 1), ('construíram', 1), ('concluiu', 1)]\n"
          ]
        }
      ]
    }
  ]
}